{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Mmh8D7EfWdM"
   },
   "source": [
    "## Predicting the Forum of origin for text data from Reddit.com using Latent Semantic analysis and k-nearest-neighbors classifier\n",
    "\n",
    "***Table of Contents***\n",
    "\n",
    "\n",
    "$\\S$**1**: Data acquisition\n",
    "\n",
    "$\\S$**2**: Data preprocessing\n",
    "\n",
    "$\\S$**3**: Model fitting\n",
    "\n",
    "$\\S$**4**: Model assessment\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\S1$\n",
    "\n",
    "Using ```praw```, or Python Reddit API Wrapper, we acquire text data from reddit.com\n",
    "\n",
    "Then, for each subreddit in ```forums```, add the posts which satisfy a chosen condition*\n",
    "\n",
    "*in this case, contains at least 100 alphabetic characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "PGkRTpEIfWdS",
    "outputId": "9aeaf2e2-476b-424b-b91d-6239a13432a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts from astrology: 106\n",
      "Number of posts from datascience: 179\n",
      "Example Post:\n",
      " Back in December when Saturn went into Capricorn I experienced major depression, major losses, and j\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import re\n",
    "\n",
    "\n",
    "# Instantiate reddit object\n",
    "reddit = praw.Reddit(client_id=\"id\",client_secret='secret',user_agent = 'Reddit Scraper')\n",
    "\n",
    "# List of subreddits to serve as our data labels \n",
    "forums = [\n",
    "    'astrology',\n",
    "    'datascience'] \n",
    "\n",
    "# Count number of alphabetic characters using RegEx substitution\n",
    "char_count = lambda post:len(re.sub('\\W|\\d','',post.selftext))\n",
    "\n",
    "# Condition for filtering the posts\n",
    "condition = lambda post: char_count(post) >= 100\n",
    "\n",
    "# Instatiate lists for data/labels and add data from each forum\n",
    "data,labels = [],[]\n",
    "for i, forum in enumerate(forums):\n",
    "    # Get latest posts from the subreddit\n",
    "    subreddit_data = reddit.subreddit(forum).new(limit=200)\n",
    "    # Filter out posts not satisfying condition\n",
    "    posts = [post.selftext for post in filter(condition,subreddit_data)]\n",
    "    # Add posts and labels to respective lists\n",
    "    data += posts\n",
    "    labels += [i]*len(posts)\n",
    "    print(f\"Number of posts from {forum}: {len(posts)}\")\n",
    "\n",
    "print(\"Example Post:\\n\",data[1][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\S2$ \n",
    "\n",
    "- Remove symbols, numbers, and url-like strings with custom preprocessor \n",
    "- vectorize text using term frequency-inverse document frequency\n",
    "- reduce to principal values using singular value decomposition\n",
    "- Partition data and labels into training/validation sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 29 samples for model testing\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\"\"\"Tune the following parameters for optimal performance\"\"\"\n",
    "MIN_DOC_FREQ = 2 # minimum frequency for a term to be used in our vectorization\n",
    "N_COMPONENTS = 1000 # number of components (words) used in our vectorization\n",
    "TEST_SIZE = .1 # percentage of total data partitioned for validation\n",
    "\n",
    "# Function to remove all non alphabetic characters/URL-like strings\n",
    "preprocessor = lambda doc: re.sub('\\W|\\d|http\\S+|www\\S+',' ',doc)\n",
    "# tf-idf vectorizer with custom preprocessing function\n",
    "vectorizer = TfidfVectorizer(preprocessor=preprocessor, stop_words='english',min_df=2)\n",
    "# SVD object to combine with vectorizer for latent semantic analysis\n",
    "decomposition = TruncatedSVD(n_components=N_COMPONENTS, n_iter=10)\n",
    "\n",
    "# Partition training/validation sets\n",
    "X_train,X_test,y_train,y_test = train_test_split(\n",
    "    data,labels,test_size=TEST_SIZE,random_state=0)\n",
    "\n",
    "print(f\"Selected {len(y_test)} samples for model testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\S3$\n",
    "\n",
    "Created k-neighbors classifier with specified ```N_NEIGHBORS``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Number of neighbors used for comparison when predicting on unseen data\n",
    "N_NEIGHBORS = 4\n",
    "\n",
    "# create classification model using k nearest neighbors\n",
    "model = KNeighborsClassifier(n_neighbors=N_NEIGHBORS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\S4$\n",
    "\n",
    "Send training set through pipeline, and evaluate model performance on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.66 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Establish pipeline\n",
    "pipe = Pipeline([\n",
    "    ('vectorizer',vectorizer),\n",
    "    ('svd',decomposition),\n",
    "    ('model',model)\n",
    "])\n",
    "\n",
    "# Send training data/labels through pipeline\n",
    "pipe.fit(X_train,y_train)\n",
    "# Predict on test data and get accuracy score\n",
    "score = pipe.score(X_test,y_test)\n",
    "print(\"Accuracy: {0:.2f} %\".format(score*100))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "EDUONIXsubreddit_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
